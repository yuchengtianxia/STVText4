# Temporal Clustering for Spatio-Temporal Video Scene Text Detection


## The proposed benchmark and the code for our approach will be publicly available at here.

## Introduction
Regardless of the temporal relation of text instances among video frames, existing video scene text detection benchmarks with only bounding-box annotation in the spatial domain are typically infeasible for application-orientation tasks, such as video text indexing and understanding, etc. In this paper, we focus on spatial-temporal video scene text detection(ST-VSTD), which targets simultaneously detecting video scene texts in both spatial and temporal domain, and has seldom been explored before. To implement ST-VSTD, we introduce a new large-scale benchmark, termed as STVText4, with a well-designed spatial-temporal detection metric (STDM) and a novel clustering-based baseline method, referred to as Temporal Clustering (TC). STVText4 consists of 141,407 video frames with more than 1.3 million text instances, where each instance is annotated with not only spatial bounding box and temporal range but also four intrinsic attributes, including legibility, density, scale, and lifecycle, to facilitate the ST-VSTD community. With continuous propagation of the identical text in the video sequence, TC extends the text detection from the spatial domain to the temporal domain and simultaneously outputs spatial quadrilateral and temporal position with the start and end boundaries. Extensive experiments demonstrate the superiority of our method and the great academic and practical value of the STVText4.
